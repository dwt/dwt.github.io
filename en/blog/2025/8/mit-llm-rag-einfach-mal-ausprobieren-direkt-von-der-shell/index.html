
<!doctype html>
<html lang="de">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="../../../../../static/bootstrap-4.1.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../../../../static/style.css">
    <link rel="stylesheet" href="../../../../../static/pygments.css">
    <title>Mit llm RAG einfach mal ausprobieren ‚Äì direkt von der Shell</title>
    <link rel="alternate" type="application/atom+xml" title="RSS: Martin H√§ckers Blog Artikel" href="../../../../../blog/feed.xml" />
  </head>
  <body>
    <header>
      <nav class="navbar navbar-expand-sm">
  <a href="../../../../" class="navbar-brand ">üè†</a>
  <input type="checkbox" id="navbar-toggle-checkbox">
  <label for="navbar-toggle-checkbox" class="navbar-brand navbar-toggle d-sm-none float-right" aria-label="Navigation Umschalten">
    <span></span>
  </label>
  <ul class="navbar-nav collapse navbar-collapse">
    
      <li class="nav-item "><a href="../../../../work/" class="nav-link">Professional software development</a></li>
    
      <li class="nav-item "><a href="../../../../projects/" class="nav-link">Projects</a></li>
    
      <li class="nav-item "><a href="../../../../publications/" class="nav-link">Publications and talks</a></li>
    
      <li class="nav-item active"><a href="../../../" class="nav-link">Blog<span class="sr-only">(ausgew√§hlt)</span></a></li>
    
      <li class="nav-item "><a href="../../../../categories/" class="nav-link">Categories</a></li>
    
    <li class="nav-item ml-auto">
      <a class=nav-link href="../../../../../blog/2025/8/mit-llm-rag-einfach-mal-ausprobieren-direkt-von-der-shell/">üá©üá™</a>
    </li>
    <li class="nav-item pull-right">
      <a class=nav-link href="./">üá¨üáß</a>
    </li>
  </ul>
</nav>
<nav class="breadcrumb">
  

  

  

<a class="breadcrumb-item " href="../../../../">üè†</a>


<a class="breadcrumb-item " href="../../../">Blog</a>


<a class="breadcrumb-item active" href="./">Mit llm RAG einfach mal ausprobieren ‚Äì direkt von der Shell</a>

</nav>
    </header>
    <article class="page mit-llm-rag-einfach-mal-ausprobieren-direkt-von-der-shell  container-fluid">
      
  
  <article class="blog-post">
    <header>
      <h2>
        
        Mit llm RAG einfach mal ausprobieren ‚Äì direkt von der Shell
        
      </h2>
      <p class="meta">
        written by Martin H√§cker on <time datetime="2025-08-07">Thursday, August 7, 2025</time>
      </p>
    </header>

    <section class="blog-post-body">
      <p><img src="workflow.png" alt="Ablaufdiagramm">
Einer der coolsten Aha-Momente der letzten Tage bei mir war, wie einfach und niederschweflig das Arbeiten mit Retrieval-Augmented Generation (RAG) von der Shell inzwischen sein kann ‚Äì ganz ohne spezielle Infrastruktur, Vektor-Datenbank oder Server-Backend. Das Python-Tool <a href="https://pypi.org/project/llm/"><code>llm</code></a> macht  das m√∂glich: RAG direkt aus der Kommandozeile, mit SQLite als Backend und einfachen Kommandos, die sich hervorragend in Shell-Workflows integrieren lassen.</p>
<h2>Ein Power Tool f√ºr Sprachmodelle in der Shell</h2>
<p><a href="https://llm.datasette.io/en/stable/"><code>llm</code> kann nat√ºrlich noch viel mehr und ist ein Power Tool, das den Einsatz von KI direkt im Terminal erlaubt</a>. Es erlaubt mit beliebigen API-Providern oder lokalen Modellen zu sprechen und integriert diese damit nahtlos in eigene Skripte. Use Cases: Daten hinein pipen und mit dem LLM bearbeiten. Ob zusammenfassen, erkl√§ren, √ºbersetzen, mit einem aufwendigen Prompt aus einer Datei beackern‚Ä¶ Da geht so viel. Egal was Ihr aus diesem Artikel mitnehmt, zumindest sollte es sein das Ihr <code>llm</code>¬†in euren Workflow aufnehmt und installiert.</p>
<h2>Der RAG-Workflow von der Shell aus</h2>
<p>Ein kompletter RAG-Workflow funktioniert mit <code>llm</code> in wenigen Schritten. <a href="https://building-with-llms-pycon-2025.readthedocs.io/en/latest/semantic-search-and-rag.html">Diese Demo hier hab ich von Simon Willison geklaut, dem Autor von <code>llm</code></a>. <a href="https://github.com/python/peps">Hier wird semantische Suche in den Python Enhancement Proposals demonstriert</a>. Dieses Tutorial verwendet einen lokalen LLM-Server, man kann das aber nat√ºrlich auch mit GitHub Copilot oder √§hnlichen Modellen machen. Herauszufinden wie die dort hei√üen und zu verbinden sind bleibt ein Exercise f√ºr den Leser.</p>
<h3>1. Dateien vorbereiten</h3>
<p>Wir erstellen gek√ºrzte Versionen von Textdateien:</p>
<div class="hll"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>peps-truncated
<span class="k">for</span><span class="w"> </span>f<span class="w"> </span><span class="k">in</span><span class="w"> </span>peps/*.rst<span class="p">;</span><span class="w"> </span><span class="k">do</span>
<span class="w">  </span>head<span class="w"> </span>-c<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$f</span><span class="s2">&quot;</span><span class="w"> </span>&gt;<span class="w"> </span><span class="s2">&quot;peps-truncated/</span><span class="k">$(</span>basename<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$f</span><span class="s2">&quot;</span><span class="k">)</span><span class="s2">&quot;</span>
<span class="k">done</span>
</pre></div>
<p>Das ist nat√ºrlich streng genommen falsch, weil wir ganz viel Daten wegschmei√üen. Einiges spricht aber trotzdem daf√ºr:</p>
<ol>
<li>Gerade lokale Modelle haben gerne nicht so gro√üe Kontext-Fenster und da muss das Dokument reinpassen.</li>
<li>Meistens steht am Anfang eines Dokuments worum es geht. F√ºr unsere Zwecke also eine gute N√§herung.</li>
</ol>
<p>Um das sp√§ter in ein Produkt umzuwandeln m√ºssten wir uns noch weitere Strategien¬†anschauen, z.B. mit einem Sliding Window √ºber die Dokumente zu gehen und f√ºr jeden Abschnitt ein Embedding zu erzeugen. Grunds√§tzlich ist es aber eine gute Idee verschiedene Indexe zu erzeugen die unterschiedliche Zwecke erf√ºllen.</p>
<h3>2. Vektor-Index erstellen</h3>
<div class="hll"><pre><span></span>llm<span class="w"> </span>embed-multi<span class="w"> </span>peps<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>mxbai-embed-large<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--files<span class="w"> </span>peps-truncated<span class="w"> </span><span class="s1">&#39;pep-3*.rst&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span>peps.db<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--store
</pre></div>
<p>Das erzeugt eine SQLite-Datenbank mit eingebetteten Vektoren. Wichtig: Die Datenbank speichert auch, mit welchem Modell die Einbettung erfolgte ‚Äì bei weiteren Operationen auf der gleichen Collection <code>peps</code> ignoriert <code>llm</code>¬†den Modell-Parameter ohne Fehlermeldung!</p>
<p>Seiten-Notiz: Embeddings, was ist das? Ein Embedding ist eine Umwandlung von einem Text in eine Zahlenreihe (Mathematisch: Einen Vektor). Jeder dieser Vektoren beschreibt eine Koordinate in einem Hoch-Dimensionalen Raum, mit der Eigenschaft, das Koordinaten die sich Nahe sind von einem Text kommen der semantisch √Ñhnlich ist. Darin kann man sogar Rechnen, ein etwas √ºberstrapaziertes Beispiel w√§re z.B. der Vektor f√ºr K√∂nig + Weiblich = K√∂nigin. <a href="https://simonwillison.net/2023/Oct/23/embeddings/">Mehr gibts hier</a></p>
<h3>3. Abfragen stellen</h3>
<div class="hll"><pre><span></span>llm<span class="w"> </span>similar<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;What do string templates look like?&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span>peps.db<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>peps<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="p">|</span><span class="w"> </span>llm<span class="w"> </span>-s<span class="w"> </span><span class="s2">&quot;Answer the question: What do string templates look like?&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>devstral<span class="w"> </span>-o<span class="w"> </span>num_ctx<span class="w"> </span>256_00
</pre></div>
<p>Hier wird erst √§hnliche Inhalte zur Frage gesucht. Wichtige Details: Das Kontext-Fenster des Sprachmodells das antwortet muss gro√ü genug sein, das es alle Antworten in seinen Kontext aufnehmen kann. Ansonsten wird gerne der Anfang abgeschnitten - und da steht nat√ºrlich der Treffer mit dem besten Score.</p>
<h3>4. Automatisieren</h3>
<p>Wir k√∂nnen sogar direkt ein Skript generieren lassen:</p>
<div class="hll"><pre><span></span>llm<span class="w"> </span><span class="s1">&#39;</span>
<span class="s1">Build me a bash script like this:</span>
<span class="s1">./pep-qa.sh &quot;What do string templates look like?&quot;</span>
<span class="s1">It should first run:</span>
<span class="s1">llm similar -c $question -d peps.db peps</span>
<span class="s1">Then it should pipe the output from that to:</span>
<span class="s1">llm -s &quot;Answer the question: $question&quot; -m gpt-4.1-mini</span>
<span class="s1">That last command should run so the output is visible as it runs.</span>
<span class="s1">&#39;</span><span class="w"> </span>-x<span class="w"> </span>&gt;<span class="w"> </span>pep-qa.sh
</pre></div>
<h2>Fazit</h2>
<p>Was fr√ºher nach viel Setup aussah (Vektor-Datenbank, Backend-API, RAG-Pipeline), ist heute in wenigen Shell-Befehlen machbar. SQLite funktioniert dabei erstaunlich gut ‚Äì <code>llm</code> f√ºhrt einfach einen Full-Table-Scan durch und berechnet die Vektor-Abst√§nde direkt. Das ist nicht hyper-skalierbar, aber bis 10.000 bis 100.000 Eintr√§ge durchaus brauchbar.</p>
<p>Und das Beste: <code>llm</code> l√§sst sich √ºberall in bestehende Shell-Skripte und Workflows einbauen ‚Äì sogar mit piped Input. Wer also mal eben eine intelligente Suche oder ein Sprachmodell in seinen CLI-Workflow integrieren will, findet hier ein extrem m√§chtiges Toolset.</p>
<p>Ich kann nur empfehlen ein Shell-Werkzeug wie <code>llm</code>¬†zu lernen.</p>

    </section>
    
  </article>

  <nav>
    
  <div class="category-list">
    Kategorien:
      
      <a href="../../../../../categories/code/">Software Development</a>
      
    
  </div>

    
    <div class="relative-navigation">
      
      
        <div class="next">N√§chstes Post: <a class="next" href="../../7/devopscon-keynote-security-ist-jetzt-teil-von-devops/">DevOpsCon Keynote ‚Äì Security ist jetzt Teil von DevOps?</a></div>
      
    </div>
    
  </nav>

    </article>
    <footer class="container-fluid ">
      <ul class="nav">
  <li class="nav-item copyright">
    <span class="nav-link">&copy; 2025 <a href="../../../../work/">Martin H√§cker</a></span>
  </li>
  
    
    <li class="nav-item imprint">
      <a class="nav-link" href="../../../../meta/#imprint">
        Imprint
      </a></li>
  
    
    <li class="nav-item privacy-policy">
      <a class="nav-link" href="../../../../meta/#privacy-policy">
        Privacy Policy
      </a></li>
  
    
    <li class="nav-item colophon">
      <a class="nav-link" href="../../../../meta/#colophon">
        Colophon
      </a></li>
  
  <li class="ml-auto nav-item rss-feed">
    <a class="nav-link" href="../../../../../blog/feed.xml">
      <img class="rss-icon" src=/static/rss.svg height=25 width=25>
      <span class="sr-only">RSS-Feed</span>
    </a>
  </li>
  <li class="nav-item license">
    <a class="nav-link" 
      rel="license" 
      target="_blank" 
      href="https://creativecommons.org/licenses/by-sa/4.0/deed"
    >
      <img 
        src="/static/cc-by-sa-88x31.png"
        alt="Creative Commons Attribution - Share Alike 4.0 International Lizense"
      >
    </a>
  </li>
</ul>
    </footer>
  </body>
</html>
